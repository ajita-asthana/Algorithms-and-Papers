# Tips for Code Refactoring 

1. Modularization: Break down logic into functions with clear responsibilities 

def load_data(file_path):
"""
Load data from either CSV or JSON format based on file extension
"""
if file_path.endswith('.csv'):    
  return pd.read_csv(file_path)
elif file_path.endswith('.json'):
  return pd.read_json(file_path)
else:
  raise ValueError(f"Unsupported file format: {file-path}")

def preprocess_data(df):
"""
Clean and prepare data for analysis
"""
return cleaned_df

def calculate_time_metrics(df):
"""
Calculate time-based metrics
"""
# Time based calculations
return df_with_metrics

2. Performance optimizations:
  * Vectored operations instead of loops
  * Efficient data structures
  * Cachching intermediary results 

3. Readability improvements
  * Clear variable naming
  * Comments for complex logic
  * Consistent formatting 

4. Error Handling 

def safe_calculation(df):
  try:
    # Calculation logic 
  except KeyError:
    # Handling missing columns 
  except ValueError:
    # Handle invalid values 

## Testing Strategy Discussion Points 
1. Unit tests for Core Functions:
  def test_calculate_time_diffs():
    # Create test data 
    test_df = pd.DataFrame({
        'user_id': [1,1,1,2,2],
        'timestamp': pd.to_datetime(['2023-01-01 10:00:00', '2023-01-01 10:05:00', 
                                      '2023-01-01 10:10:00', '2023-01-01 11:00:00', 
                                      '2023-01-01 11:30:00'])
    })

  # Apply Function
  result = calculate_time_diffs(test_df)

  # Assert expected values 
  assert result.loc[1, 'time_since_last'] == 300.0 


## Edge Case Testing 

1. Empty datasets
2. Missing Values
3. Extreme Values
4. Duplicate timestamps 
5. Data spanning multiple days/months 

## Integration testing 

1. End-to-End pipeline testing 
2. Verifying outputs match expected formats 

## Performance Testing:

1. How solution scales with larger datasets 
2. Memory usage considerations 

## Common pitfalls to avoid 
1. Not handling missing or invalid data
2. Time zone confusion ---> Be clear about UTC vs local time
3. Inefficient operations within loops instead of vectorized operations 
4. Memory issues with large datasets 
5. Incorrect grouping that loses data fidelity 
6. Not explaining your thought process as you work through the problem 

## Format Specific Tips:

1. Check for header row: Sometimes CSV files might have multiple header rows or missing headers 
  # Custom header handling 
  df = pd.read_csv('file.csv', sep='|')
  df.columns = ['col1', 'col2', 'col3']

## JSON processing tips 

normalized_df = pd.json_normalize(
                      data, record_path=['nested_array'],
                            meta =['user_id', 'session_id']


2. Work with complex JSON paths:
df['nested_value'] = df['parent_field'].apply(lambds x:x.get('child'm {}).get('value')

3. Handling inconsistent structures: Not all JSON objects may have the same fields 

def safe_get(obj, *keys):
    """
    Safely navigate nested dictionaries with potential missing keys 
    """
for key in keys:
  try:
    obj = obj[key]
  except (KeyError, TypeError):
    return None 
return obj

df['extracted'] = df['data'].apply(lambda x: safe_get(x, 'nested', 'deeply', 'value'))
